{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# create dataset from a random, two-dimensional distribution\n",
    "N = 100000\n",
    "X, y = make_blobs(\n",
    "    n_samples=N, n_features=2, centers=[[0, 0]], cluster_std=3, random_state=2\n",
    ")\n",
    "\n",
    "# NOTE: mins and maxs are global variables so the renderer uses the same axes across plots\n",
    "mins = X.min(axis=0)\n",
    "maxs = X.max(axis=0)\n",
    "\n",
    "\n",
    "def add_noise(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    X2 = np.random.rand(int(N * 0.25), 2)\n",
    "    y2 = np.full(int(N * 0.25), -1)\n",
    "    X2[:, 0] = (X2[:, 0] * (maxs[0] - mins[0])) + mins[0]\n",
    "    X2[:, 1] = (X2[:, 1]) * (maxs[1] - mins[1]) + mins[1]\n",
    "\n",
    "    X_ = np.concatenate([X, X2], axis=0)\n",
    "    y_ = np.concatenate([y, y2], axis=0)\n",
    "    return X_, y_\n",
    "\n",
    "\n",
    "# add \"background noise\" to the blobs to clarify the equal-density sample\n",
    "X, y = add_noise(X, y)\n",
    "\n",
    "\n",
    "def cluster(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Performs a simple dbscan clustering on the input dataset, returning the labels.\n",
    "    \"\"\"\n",
    "    # NOTE: in dbscan: -1 is outlier, so inlier is > -1\n",
    "    clustering = DBSCAN(eps=0.35, min_samples=100).fit(X)\n",
    "    c = clustering.labels_\n",
    "    return c\n",
    "\n",
    "\n",
    "def to_df(X: np.ndarray, y: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms the input dataset and labels into a DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(X, columns=[\"A\", \"B\"])\n",
    "    df[\"y\"] = y\n",
    "    return df\n",
    "\n",
    "\n",
    "def draw_random_sample(X: np.ndarray, y: np.ndarray, n_sample: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a random (uniform) sample of the input data.\n",
    "    \"\"\"\n",
    "    sample = sample_without_replacement(\n",
    "        n_population=len(X), n_samples=n_sample, random_state=0\n",
    "    )\n",
    "\n",
    "    return to_df(X[sample], y[sample])\n",
    "\n",
    "\n",
    "def draw_uniform_sample(X: np.ndarray, y: np.ndarray, n_sample: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster the data and then draw a random uniform sample.\n",
    "    \"\"\"\n",
    "    c = cluster(X)\n",
    "    c = (c > -1).astype(int)\n",
    "\n",
    "    return draw_random_sample(X, c, n_sample)\n",
    "\n",
    "\n",
    "def draw_inlier_sample(X: np.ndarray, y: np.ndarray, n_sample: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster the data and then only sample from inliers.\n",
    "    \"\"\"\n",
    "    # dbscan: -1 is outlier, so inlier is > -1\n",
    "    c = cluster(X)\n",
    "    c = (c > -1).astype(int)\n",
    "\n",
    "    # in case there are more than n_sample inliers, sample down uniformly\n",
    "    return draw_random_sample(X[c == 1], c[c == 1], n_sample=n_sample)\n",
    "\n",
    "\n",
    "def draw_outlier_sample(X: np.ndarray, y: np.ndarray, n_sample: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster the data and then only sample from outliers.\n",
    "    \"\"\"\n",
    "    c = cluster(X).astype(int)\n",
    "\n",
    "    return draw_random_sample(X[c == -1], c[c == -1], n_sample)\n",
    "\n",
    "\n",
    "def draw_stratified_sample(X: np.ndarray, y: np.ndarray, n_sample: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster the data and then sample equal number of points from inlier and outlier classes.\n",
    "    \"\"\"\n",
    "    c = cluster(X)\n",
    "    c = (c > -1).astype(int)\n",
    "\n",
    "    # clustering might find more than 2 clusters, but all we need is inlier/outliers\n",
    "    rus = RandomUnderSampler()\n",
    "    X_, y_ = rus.fit_resample(X, c)\n",
    "    return draw_random_sample(X_, y_, n_sample=n_sample)\n",
    "\n",
    "\n",
    "def draw_noise_sample(X: np.ndarray, y: np.ndarray, n_sample: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster the data and then draw a random sample that has approximately the same density \n",
    "    everywhere.\n",
    "    \"\"\"\n",
    "    c = cluster(X)\n",
    "    c = (c > -1).astype(int)\n",
    "\n",
    "    return draw_random_sample(X[y == -1], c[y == -1], n_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=[\"A\", \"B\"])\n",
    "plt = df.plot.hexbin(\n",
    "  x='A',\n",
    "  y='B',\n",
    "  reduce_C_function=np.sum,\n",
    "  gridsize=100,\n",
    "  cmap=\"viridis\",\n",
    "  figsize=[14, 12],\n",
    ")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.set_xlim([mins[0], maxs[0]])\n",
    "plt.set_ylim([mins[1], maxs[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot different samplings from the dataset (*Note: comment/uncomment the first few lines depending on which sample you want*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a sampling\n",
    "df = draw_uniform_sample(X, y, 10000)\n",
    "# df = draw_noise_sample(X, y, 10000)\n",
    "# df = draw_stratified_sample(X, y, 10000)\n",
    "# df = draw_inlier_sample(X, y, 10000)\n",
    "# df = draw_outlier_sample(X, y, 10000)\n",
    "\n",
    "plt = df.plot.scatter(\n",
    "    x=\"A\",\n",
    "    y=\"B\",\n",
    "    # c=\"#2f124b\",  # use this one for inlier sampling\n",
    "    c=\"y\",\n",
    "    colormap=\"PuOr\",\n",
    "    alpha=0.3,\n",
    "    figsize=[12, 12],\n",
    "    colorbar=False,\n",
    ")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.set_xlim([mins[0], maxs[0]])\n",
    "plt.set_ylim([mins[1], maxs[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendering functions used in the following examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(f\"{os.getcwd()}/../\")  # necessary to import Pipeline classes in this notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pipeline.Pipeline import Pipeline\n",
    "\n",
    "columns = [\n",
    "    \"tripID\",\n",
    "    \"VendorID\",\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"RatecodeID\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"payment_type\",\n",
    "    \"fare_amount\",\n",
    "    \"extra\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"improvement_surcharge\",\n",
    "    \"total_amount\",\n",
    "    \"PURepresentativeX\",\n",
    "    \"PURepresentativeY\",\n",
    "    \"DORepresentativeX\",\n",
    "    \"DORepresentativeY\",\n",
    "    \"normalized_value\",\n",
    "    \"normalized_spatial_lag\",\n",
    "    \"value_is_H\",\n",
    "    \"spatial_lag_is_H\",\n",
    "]\n",
    "\n",
    "gt_df = pd.read_csv(\n",
    "    \"./linearization_files/taxisLinearizationRandom.csv\", sep=\";\", names=columns\n",
    ")\n",
    "gt_df[\"tpep_pickup_datetime\"] = pd.to_datetime(gt_df[\"tpep_pickup_datetime\"])\n",
    "\n",
    "n_bins = 100\n",
    "\n",
    "\n",
    "def plot_delta_hist(df: pd.DataFrame, facet: str, attr: str = \"trip_distance\"):\n",
    "    cases = df[facet].unique()\n",
    "    # make sure there are at least two plots so that the ax[i] works\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=cases.size, figsize=(len(cases) * 10, 3.5))\n",
    "\n",
    "    df_ = gt_df[attr]\n",
    "\n",
    "    if type(gt_df[attr].iloc[0]) == pd.Timestamp:\n",
    "        bins = pd.date_range(df_.min(), df_.max(), periods=n_bins + 1)\n",
    "    else:\n",
    "        bins = np.linspace(\n",
    "            df_.min(), df_.max(), n_bins + 1\n",
    "        )  # because arange does not include max\n",
    "\n",
    "    gt_bins = np.histogram(df_, bins=bins)[0]\n",
    "    gt_bins = gt_bins / gt_bins.sum()\n",
    "\n",
    "    for i, case in enumerate(cases):\n",
    "        # compute the relative difference in value distributions from the \"ground truth\"\n",
    "        df_ = df[df[facet] == case]\n",
    "        df_bins = np.histogram(df_[attr], bins=bins)[0]\n",
    "        df_bins = df_bins / df_bins.sum()\n",
    "\n",
    "        delta = df_bins - gt_bins\n",
    "        color = [\"#1b9e77\" if value > 0 else \"#7570b3\" for value in delta]\n",
    "\n",
    "        ax_ = ax[i] if len(cases) > 1 else ax\n",
    "        ax_.bar(\n",
    "            np.arange(0, len(delta)),\n",
    "            height=delta,\n",
    "            color=color,\n",
    "            width=1,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "        ax_.set_ylim(-0.5, 0.5)\n",
    "        ax_.get_xaxis().set_visible(False)\n",
    "        ax_.get_yaxis().set_visible(False) if i > 0 else None\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_hist(df: pd.DataFrame, facet: str, attr: str = \"trip_distance\"):\n",
    "    g = sns.FacetGrid(\n",
    "        data=df,\n",
    "        col=facet,\n",
    "        palette=\"colorblind\",\n",
    "        height=3.5,\n",
    "        aspect=2.8,\n",
    "    )\n",
    "\n",
    "    min_value = pd.to_numeric(gt_df[attr]).min()\n",
    "    max_value = pd.to_numeric(gt_df[attr]).max()\n",
    "\n",
    "    g.map(\n",
    "        sns.histplot,\n",
    "        attr,\n",
    "        bins=n_bins,\n",
    "        binrange=[min_value, max_value],\n",
    "        # log_scale=True,\n",
    "        legend=True,\n",
    "    )\n",
    "\n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=f\"{facet}\" + \": {row_name}\")\n",
    "    g.fig.suptitle(facet, y=1)\n",
    "    g.add_legend()\n",
    "    g.fig.show()\n",
    "\n",
    "\n",
    "def plot_multi_hist(df: pd.DataFrame, facet: str):\n",
    "    plot_hist(df, facet, \"trip_distance\")\n",
    "    plot_hist(df, facet, \"tpep_pickup_datetime\")\n",
    "\n",
    "\n",
    "def plot_multi_delta_hist(df: pd.DataFrame, facet: str):\n",
    "    plot_delta_hist(df, facet, \"trip_distance\")\n",
    "    plot_delta_hist(df, facet, \"tpep_pickup_datetime\")\n",
    "\n",
    "\n",
    "def plot_geo(df: pd.DataFrame, facet: str, attr: str = \"trip_distance\"):\n",
    "    df[\"group\"] = pd.qcut(df[attr].astype(np.float64), q=4)\n",
    "\n",
    "    g = sns.relplot(\n",
    "        data=df,\n",
    "        x=\"PURepresentativeX\",\n",
    "        y=\"PURepresentativeY\",\n",
    "        col=facet,\n",
    "        row=\"group\",\n",
    "        kind=\"scatter\",\n",
    "        hue=attr,\n",
    "        # size=0.1,\n",
    "        palette=\"viridis\",\n",
    "        height=4,\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    g.fig.suptitle(facet, y=1)\n",
    "    g.add_legend()\n",
    "\n",
    "\n",
    "def plot_geo_dist(df: pd.DataFrame, facet: str):\n",
    "    # sns.set_palette(\"ch:start=.2,rot=-.3\")\n",
    "    g = sns.FacetGrid(\n",
    "        data=df,\n",
    "        col=facet,\n",
    "        height=5,\n",
    "    )\n",
    "    g.map_dataframe(\n",
    "        sns.histplot,\n",
    "        x=\"PURepresentativeX\",\n",
    "        y=\"PURepresentativeY\",\n",
    "        bins=100,\n",
    "        legend=True,\n",
    "    )\n",
    "    # sns.set_palette(\"ch:start=.2,rot=-.3\")\n",
    "\n",
    "\n",
    "def plot_moran_dist(df: pd.DataFrame, facet: str):\n",
    "    df[\"moran_label\"] = df[\"value_is_H\"].astype(str) + df[\"spatial_lag_is_H\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    g = sns.FacetGrid(df, col=facet, height=1.5, aspect=2.5)\n",
    "    g.map_dataframe(sns.histplot, y=\"moran_label\")\n",
    "\n",
    "\n",
    "def plot_moran(df: pd.DataFrame, facet: str):\n",
    "    df[\"moran_label\"] = df[\"value_is_H\"].astype(str) + df[\"spatial_lag_is_H\"].astype(\n",
    "        str\n",
    "    )\n",
    "\n",
    "    g = sns.relplot(\n",
    "        data=df,\n",
    "        x=\"normalized_value\",\n",
    "        y=\"normalized_spatial_lag\",\n",
    "        hue=\"moran_label\",\n",
    "        col=facet,\n",
    "        kind=\"scatter\",\n",
    "        # size=0.1,\n",
    "        height=4,\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    g.refline(x=0, y=0, color=\"black\")\n",
    "\n",
    "\n",
    "def render_chunk(df: pd.DataFrame, facet: str or list[str]):\n",
    "    if type(facet) == list:\n",
    "        facets = facet.copy()\n",
    "        facet = \"--\".join(facet)\n",
    "        df[facet] = \"\"\n",
    "        for f in facets:\n",
    "            df[facet] += df[f].astype(str) + \" \"\n",
    "\n",
    "    plot_multi_hist(df, facet)\n",
    "    # plot_geo(df, facet)\n",
    "    plot_geo_dist(df, facet)\n",
    "    # FIXME: the next line makes sure that delta histograms work\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    plot_multi_delta_hist(df, facet)\n",
    "    plot_moran_dist(df, facet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"data\": \"taxis\",\n",
    "    \"linearization\": \"random\",\n",
    "    \"subdivision\": \"cardinality\",\n",
    "    \"selection\": \"maximum\",\n",
    "    \"dimension\": 5,  # trip distance\n",
    "    \"params\": {\n",
    "        \"subspace\": [17, 18],  # pickup location,\n",
    "        # \"coverage\": 16,  # total amount\n",
    "        \"coverage\": 5,  # trip distance\n",
    "        \"value_h_index\": 23,\n",
    "        \"lag_h_index\": 24,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline(base_config)\n",
    "\n",
    "chunk = pd.DataFrame(pl.get_next_chunk(1000), columns=columns)\n",
    "chunk[\"linearization\"] = base_config[\"linearization\"]\n",
    "chunk[\"subdivision\"] = base_config[\"subdivision\"]\n",
    "chunk[\"selection\"] = base_config[\"selection\"]\n",
    "\n",
    "render_chunk(chunk, \"linearization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the Linearization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "linearizations = [\"random\", \"z-order\", \"numeric\"]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for linearization in linearizations:\n",
    "    config = base_config.copy()\n",
    "    config[\"linearization\"] = linearization\n",
    "\n",
    "    pl = Pipeline(config)\n",
    "\n",
    "    chunk = pd.DataFrame(pl.get_next_chunk(10000), columns=columns)\n",
    "    chunk[\"linearization\"] = config[\"linearization\"]\n",
    "    chunk[\"subdivision\"] = config[\"subdivision\"]\n",
    "    chunk[\"selection\"] = config[\"selection\"]\n",
    "\n",
    "    df = pd.concat([df, chunk], ignore_index=True)\n",
    "\n",
    "render_chunk(df, \"linearization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the Subdivision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "subdivisions = [\"cardinality\", \"cohesion\", \"coverage\"]\n",
    "\n",
    "base_config_ = base_config.copy()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for subdivision in subdivisions:\n",
    "  config = base_config_.copy()\n",
    "\n",
    "  config[\"subdivision\"] = subdivision\n",
    "  pl = Pipeline(config)\n",
    "\n",
    "  chunk = pd.DataFrame(pl.get_next_chunk(10000), columns=columns)\n",
    "  chunk[\"linearization\"] = config[\"linearization\"]\n",
    "  chunk[\"subdivision\"] = config[\"subdivision\"]\n",
    "  chunk[\"selection\"] = config[\"selection\"]\n",
    "\n",
    "  df = pd.concat([df, chunk], ignore_index=True)\n",
    "\n",
    "render_chunk(df, \"subdivision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "selections = [\"maximum\", \"median\", \"random\"]\n",
    "\n",
    "base_config_ = base_config.copy()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for selection in selections:\n",
    "  config = base_config.copy()\n",
    "  config[\"selection\"] = selection\n",
    "  pl = Pipeline(config)\n",
    "\n",
    "  chunk = pd.DataFrame(pl.get_next_chunk(3333), columns=columns)\n",
    "  chunk[\"linearization\"] = config[\"linearization\"]\n",
    "  chunk[\"subdivision\"] = config[\"subdivision\"]\n",
    "  chunk[\"selection\"] = config[\"selection\"]\n",
    "\n",
    "  df = pd.concat([df, chunk], ignore_index=True)\n",
    "\n",
    "render_chunk(df, \"selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect of changing chunk size on selection runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "config = base_config.copy()\n",
    "config[\"subdivision\"] = \"cohesion\"  # creates \"uneven\" number of bins\n",
    "config[\"selection\"] = \"median\"  # complex selection strategy\n",
    "\n",
    "pl = Pipeline(config)\n",
    "print(f\"preprocessing took {time.time() - start}s\")\n",
    "\n",
    "chunk_sizes = [100, 313, 1000, 1763, 2000, 5000, 10000, 56721, 100000]\n",
    "\n",
    "runtimes = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "  now = time.time()\n",
    "  chunk = pd.DataFrame(pl.get_next_chunk(chunk_size), columns=columns)\n",
    "  runtime = time.time() - now\n",
    "  runtimes += [(chunk_size, runtime)]\n",
    "\n",
    "print(\"\\ntotal\", time.time() - start)\n",
    "pd.DataFrame(runtimes, columns=[\"chunk_size\", \"runtime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating existing sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "random_config = {\n",
    "  \"data\": \"taxis\",\n",
    "  \"linearization\": \"random\",\n",
    "  \"subdivision\": \"cardinality\",\n",
    "  \"selection\": \"random\",\n",
    "  \"dimension\": -1,  # not used\n",
    "  \"params\": {}\n",
    "}\n",
    "\n",
    "stratified_config = {\n",
    "  \"data\": \"taxis\",\n",
    "  \"linearization\": \"numeric\",\n",
    "  \"subdivision\": \"interval\",\n",
    "  \"selection\": \"random\",\n",
    "  \"dimension\": 5,  # trip distance\n",
    "  \"params\": {\n",
    "    \"subspace\": [5],  # trip distance\n",
    "  }\n",
    "}\n",
    "\n",
    "autocorrelation_config = {\n",
    "  \"data\": \"taxis\",\n",
    "  \"linearization\": \"z-order\",\n",
    "  \"subdivision\": \"cardinality\",\n",
    "  \"selection\": \"autocorrelation\",\n",
    "  \"dimension\": 5,  # trip distance\n",
    "  \"params\": {\n",
    "    \"subspace\": [17, 18],  # pickup location\n",
    "    \"value_h_index\": 23,  # mean h category\n",
    "    \"lag_h_index\": 24  # lagged h category\n",
    "  }\n",
    "}\n",
    "\n",
    "combinations = [\n",
    "  (\"random\", random_config),\n",
    "  (\"stratified\", stratified_config),\n",
    "  (\"autocorrelation\", autocorrelation_config),\n",
    "]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for name, config in combinations:\n",
    "  pl = Pipeline(config)\n",
    "\n",
    "  chunk = pd.DataFrame(pl.get_next_chunk(10000), columns=columns)\n",
    "\n",
    "  chunk[\"linearization\"] = config[\"linearization\"]\n",
    "  chunk[\"subdivision\"] = config[\"subdivision\"]\n",
    "  chunk[\"selection\"] = config[\"selection\"]\n",
    "\n",
    "  df = pd.concat([df, chunk], ignore_index=True)\n",
    "\n",
    "render_chunk(df, [\"linearization\", \"subdivision\", \"selection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tailoring the sampling to multiple attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_config_ = base_config.copy()\n",
    "base_config_[\"selection\"] = \"random\"\n",
    "\n",
    "multiple_attr_config = {\n",
    "  \"data\": \"taxis\",\n",
    "  \"linearization\": \"z-order\",\n",
    "  \"subdivision\": \"coverage\",\n",
    "  \"selection\": \"maximum\",\n",
    "  # \"dimension\": 5,  # trip distance\n",
    "  \"dimension\": 2,  # pickup time\n",
    "  \"params\": {\n",
    "    \"subspace\": [17, 18],  # pickup location,\n",
    "    # \"coverage\": 16,  # total amount\n",
    "    \"coverage\": 5,  # trip distance\n",
    "    # \"coverage\": 2,  # pickup time\n",
    "    \"value_h_index\": 23,\n",
    "    \"lag_h_index\": 24,\n",
    "  }\n",
    "}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "configs = [base_config_, multiple_attr_config]\n",
    "\n",
    "for config in configs:\n",
    "  pl = Pipeline(config)\n",
    "\n",
    "  chunk = pd.DataFrame(pl.get_next_chunk(10000), columns=columns)\n",
    "  chunk[\"linearization\"] = config[\"linearization\"]\n",
    "  chunk[\"subdivision\"] = config[\"subdivision\"]\n",
    "  chunk[\"selection\"] = config[\"selection\"]\n",
    "\n",
    "  df = pd.concat([df, chunk], ignore_index=True)\n",
    "\n",
    "render_chunk(df, [\"linearization\", \"subdivision\", \"selection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing operators for tailoring the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposing_pipelines_config = {\n",
    "  \"data\": \"taxis\",\n",
    "  \"linearization\": \"z-order\",\n",
    "  \"subdivision\": \"cohesion\",\n",
    "  \"selection\": \"random\",\n",
    "  \"dimension\": 5,  # spatial autocorrelation\n",
    "  \"params\": {\n",
    "    \"subspace\": [17, 18],  # pickup location,\n",
    "    # \"coverage\": 16,  # total amount\n",
    "    \n",
    "    \"coverage\": 5,  # trip distance\n",
    "    \"value_h_index\": 23,\n",
    "    \"lag_h_index\": 24,\n",
    "  }\n",
    "}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "configs = [base_config_, recomposing_pipelines_config]\n",
    "\n",
    "for config in configs:\n",
    "  pl = Pipeline(config)\n",
    "\n",
    "  chunk = pd.DataFrame(pl.get_next_chunk(10000), columns=columns)\n",
    "  chunk[\"linearization\"] = config[\"linearization\"]\n",
    "  chunk[\"subdivision\"] = config[\"subdivision\"]\n",
    "  chunk[\"selection\"] = config[\"selection\"]\n",
    "\n",
    "  df = pd.concat([df, chunk], ignore_index=True)\n",
    "\n",
    "render_chunk(df, [\"linearization\", \"subdivision\", \"selection\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1ea470226d0cff7032cd0b9e9385b52bdf95c0a60e2d883ab49f34ab0debaa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
